{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heart Disease Prediction\n",
    "Author: [Usman Tariq](https://github.com/USM811)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;<a id=\"content_list\">**Content List**</a>\n",
    "> 1.0 - [Introduction](#intro)\\\n",
    "> &emsp;1.1 - [Purpose](#purpose)\\\n",
    "> &emsp;1.2 - [Methodology](#methodology)\\\n",
    "> \\\n",
    "> 2.0 - [About Data](#about_data)\\\n",
    "> &emsp;2.1 - [Context](#about_data)\\\n",
    "> &emsp;2.2 - [Column Descriptions](#column_descriptions)\\\n",
    "> \\\n",
    "> 3.0 - [Acknowledgements](#acknowledgements)\\\n",
    "> &emsp;3.1 - [Creators](#creators)\\\n",
    "> \\\n",
    "> 4.0 - [Import Libraries](#import_libraries)\\\n",
    "> \\\n",
    "> 5.0 - [Load Dataset](#loading_dataset)\\\n",
    "> \\\n",
    "> 6.0 - [Data Overview](#data_overview)\\\n",
    "> &emsp;6.1 - [Inspect Data Dimensions](#dimenstions)\\\n",
    "> &emsp;6.2 - [Inspect Missing Data](#inspect_missing)\\\n",
    "> &emsp;6.3 - [Inspect Categorical Features](#inspect_cat_cols)\\\n",
    "> &emsp;6.4 - [Inspect Numerical Features](#inspect_num_cols)\\\n",
    "> \\\n",
    "> 7.0 - [Data Pre-processing](#data_preprocessing)\\\n",
    "> &emsp;7.1 - [Drop the Irrelevant Features](#irrelevant_cols)\\\n",
    "> &emsp;7.2 - [Handling Missing Data](#handle_missing)\\\n",
    "> &emsp;&emsp;7.2.1 - [Impute the `Numerical Features` with low (`less then 10%`) Missing Values](#handle_num_cols)\\\n",
    "> &emsp;&emsp;7.2.2 - [Impute the Categorical Features with low (`less then 10%`) Missing Values](#handle_cat_cols)\\\n",
    "> &emsp;&emsp;7.2.3 - [Impute the Features with `high percentage` of Missing Values](#handle_high_missing_cols)\\\n",
    "> &emsp;[Observations and Improvements - 7.2](#observation_improvements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"intro\">1.0 - Introduction</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;<a id=\"purpose\">**1.1 - Purpose**</a>\n",
    "> In this notebook, I constructed a Machine Learning model to `deal the missing data` by carefully looking at the various madical attributes provided in the [`UCI Heart Disease Dataset`](https://www.kaggle.com/datasets/redwankarimsony/heart-disease-data/data).\n",
    ">\n",
    "> My ultimate goal has been to predict the intensity of the heart disease in human body by measuring different medical parameters (`i.e. chest pain, blood pressure, cholesterol, heart beat rate, etc.`), based on its `age` and `gender`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;<a id=\"methodology\">**1.2 - Methodlogy**</a>\n",
    "+ The following machine learning algorithms will be used to build the model:\n",
    "> 1. Logistic Regression\n",
    "> 2. Support Vector Machine (SVC)\n",
    "> 3. K-Nearest Neighbors (KNN)\n",
    "> 4. Decision Tree Algorithm\n",
    "> 5. Random Forest Algorithm\n",
    "\n",
    "+ The following metrics will be used to measure the model performance:\n",
    "> 1. Confusion Matrix\n",
    "> 2. Classification Report\n",
    "> 3. F1 Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='text-align: center;'>\n",
    "Back to the <a href=\"#content_list\">Content List</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"about_data\">2.0 - About Data</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;<a id=\"context\">**2.1 - Context**</a>\n",
    "> This is a multivariate type of dataset which means providing or involving a variety of separate mathematical or statistical variables, multivariate numerical data analysis. It is composed of 14 attributes which are age, sex, chest pain type, resting blood pressure, serum cholesterol, fasting blood sugar, resting electrocardiographic results, maximum heart rate achieved, exercise-induced angina, oldpeak â€” ST depression induced by exercise relative to rest, the slope of the peak exercise ST segment, number of major vessels and Thalassemia. This database includes 76 attributes, but all published studies relate to the use of a subset of 14 of them. The Cleveland database is the only one used by ML researchers to date. One of the major tasks on this dataset is to predict based on the given attributes of a patient that whether that particular person has heart disease or not and other is the experimental task to diagnose and find out various insights from this dataset which could help in understanding the problem more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;<a id=\"column_descriptions\">**2.2 - Column Descriptions**</a>\n",
    "> 01. `id`: (Unique id for each patient)\n",
    "> 02. `age`: (Age of the patient in years)\n",
    "> 03. `origin`: (place of study)\n",
    "> 04. `sex`: (Male/Female)\n",
    "> 05. `cp`: chest pain type ([typical angina, atypical angina, non-anginal, asymptomatic])\n",
    "> 06. `trestbps`: resting blood pressure (resting blood pressure (in mm Hg on admission to the hospital))\n",
    "> 07. `chol`: (serum cholesterol in mg/dl)\n",
    "> 08. `fbs`: (if fasting blood sugar > 120 mg/dl)\n",
    "> 09. `restecg`: (resting electrocardiographic results) -- Values: [normal, stt abnormality, lv hypertrophy]\n",
    "> 10. `thalach`: maximum heart rate achieved\n",
    "> 11. `exang`: exercise-induced angina (True/ False)\n",
    "> 12. `oldpeak`: ST depression induced by exercise relative to rest\n",
    "> 13. `slope`: the slope of the peak exercise ST segment\n",
    "> 14. `ca`: number of major vessels (0-3) colored by fluoroscopy\n",
    "> 15. `thal`: [normal; fixed defect; reversible defect]\n",
    "> 16. `num`: the predicted attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='text-align: center;'>\n",
    "Back to the <a href=\"#content_list\">Content List</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"acknowledgements\"> 3.0 - Acknowledgements</a>\n",
    "&emsp;<a id=\"creators\">**3.1 - Creators**</a>\n",
    "> 1. Hungarian Institute of Cardiology. Budapest: Andras Janosi, M.D.\n",
    "> 2. University Hospital, Zurich, Switzerland: William Steinbrunn, M.D.\n",
    "> 3. University Hospital, Basel, Switzerland: Matthias Pfisterer, M.D.\n",
    "> 4. V.A. Medical Center, Long Beach and Cleveland Clinic Foundation: Robert Detrano, M.D., Ph.D."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"import_libraries\">4.0 - Import Libraries</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# import label encoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# import metrics\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# import warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"loading_dataset\">5.0 - Load Dataset</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Dataset\n",
    "df = pd.read_csv('heart_disease_uci.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"data_overview\">6.0 - Data Overview</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;<a id=\"dimenstions\">**6.1 - Inspect Data Dimensions**</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>dataset</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalch</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>63</td>\n",
       "      <td>Male</td>\n",
       "      <td>Cleveland</td>\n",
       "      <td>typical angina</td>\n",
       "      <td>145.0</td>\n",
       "      <td>233.0</td>\n",
       "      <td>True</td>\n",
       "      <td>lv hypertrophy</td>\n",
       "      <td>150.0</td>\n",
       "      <td>False</td>\n",
       "      <td>2.3</td>\n",
       "      <td>downsloping</td>\n",
       "      <td>0.0</td>\n",
       "      <td>fixed defect</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>67</td>\n",
       "      <td>Male</td>\n",
       "      <td>Cleveland</td>\n",
       "      <td>asymptomatic</td>\n",
       "      <td>160.0</td>\n",
       "      <td>286.0</td>\n",
       "      <td>False</td>\n",
       "      <td>lv hypertrophy</td>\n",
       "      <td>108.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.5</td>\n",
       "      <td>flat</td>\n",
       "      <td>3.0</td>\n",
       "      <td>normal</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>67</td>\n",
       "      <td>Male</td>\n",
       "      <td>Cleveland</td>\n",
       "      <td>asymptomatic</td>\n",
       "      <td>120.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>False</td>\n",
       "      <td>lv hypertrophy</td>\n",
       "      <td>129.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2.6</td>\n",
       "      <td>flat</td>\n",
       "      <td>2.0</td>\n",
       "      <td>reversable defect</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>37</td>\n",
       "      <td>Male</td>\n",
       "      <td>Cleveland</td>\n",
       "      <td>non-anginal</td>\n",
       "      <td>130.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>False</td>\n",
       "      <td>normal</td>\n",
       "      <td>187.0</td>\n",
       "      <td>False</td>\n",
       "      <td>3.5</td>\n",
       "      <td>downsloping</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>41</td>\n",
       "      <td>Female</td>\n",
       "      <td>Cleveland</td>\n",
       "      <td>atypical angina</td>\n",
       "      <td>130.0</td>\n",
       "      <td>204.0</td>\n",
       "      <td>False</td>\n",
       "      <td>lv hypertrophy</td>\n",
       "      <td>172.0</td>\n",
       "      <td>False</td>\n",
       "      <td>1.4</td>\n",
       "      <td>upsloping</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  age     sex    dataset               cp  trestbps   chol    fbs  \\\n",
       "0   1   63    Male  Cleveland   typical angina     145.0  233.0   True   \n",
       "1   2   67    Male  Cleveland     asymptomatic     160.0  286.0  False   \n",
       "2   3   67    Male  Cleveland     asymptomatic     120.0  229.0  False   \n",
       "3   4   37    Male  Cleveland      non-anginal     130.0  250.0  False   \n",
       "4   5   41  Female  Cleveland  atypical angina     130.0  204.0  False   \n",
       "\n",
       "          restecg  thalch  exang  oldpeak        slope   ca  \\\n",
       "0  lv hypertrophy   150.0  False      2.3  downsloping  0.0   \n",
       "1  lv hypertrophy   108.0   True      1.5         flat  3.0   \n",
       "2  lv hypertrophy   129.0   True      2.6         flat  2.0   \n",
       "3          normal   187.0  False      3.5  downsloping  0.0   \n",
       "4  lv hypertrophy   172.0  False      1.4    upsloping  0.0   \n",
       "\n",
       "                thal  num  \n",
       "0       fixed defect    0  \n",
       "1             normal    2  \n",
       "2  reversable defect    1  \n",
       "3             normal    0  \n",
       "4             normal    0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display first 5 rows.\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(920, 16)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape of the dataset\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'age', 'sex', 'dataset', 'cp', 'trestbps', 'chol', 'fbs',\n",
       "       'restecg', 'thalch', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'num'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Columns of the dataset\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp; **Observations - 6.1**\n",
    "> + There are `920` rows, means the data of `920` human being.\n",
    "> + There are total `16` columns in the dataset, including `id`, `dataset (location of the patient)`.\n",
    "> + The target feature `num` represents the ordinal numeric severity of the heart disease (`[0, 1, 2, 3, 4]`).\n",
    "> + There are `13` features or `medical parameters` (excluding `id` and `dataset`), which will be used to predict the target feature `num` (the intensity of the heart disease)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;<a id=\"inspect_missing\">**6.2 - Inspect Missing Data**</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ca          66.4\n",
       "thal        52.8\n",
       "slope       33.6\n",
       "fbs          9.8\n",
       "oldpeak      6.7\n",
       "trestbps     6.4\n",
       "thalch       6.0\n",
       "exang        6.0\n",
       "chol         3.3\n",
       "restecg      0.2\n",
       "dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identify the columns in which the data is missing.\n",
    "round((df.isnull().sum()[df.isnull().sum()>0]/len(df)*100),1).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;**Observations - 6.2**\n",
    "> + There are `10` features in which the data is missing.\n",
    "> + There are `7` features in which the `percentage of missing data` is `less than 10%`\n",
    "> + There are `3` features in which the `percentaage of missing data` is `high (33%, 52%, 66%)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;<a id=\"inspect_cat_cols\">**6.3 - Inspect Categorical Features**</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sex : ['Male' 'Female'] \n",
      "\n",
      "dataset : ['Cleveland' 'Hungary' 'Switzerland' 'VA Long Beach'] \n",
      "\n",
      "cp : ['typical angina' 'asymptomatic' 'non-anginal' 'atypical angina'] \n",
      "\n",
      "fbs : [True False nan] \n",
      "\n",
      "restecg : ['lv hypertrophy' 'normal' 'st-t abnormality' nan] \n",
      "\n",
      "exang : [False True nan] \n",
      "\n",
      "slope : ['downsloping' 'flat' 'upsloping' nan] \n",
      "\n",
      "thal : ['fixed defect' 'normal' 'reversable defect' nan] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Identify the unique values in each categorical column.\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == 'object' or df[col].dtype == 'category':\n",
    "        print(col, \":\", df[col].unique(), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;**Observations - 6.3**\n",
    "> + There are no spelling mistakes in the categorical-values of the categorical features.\n",
    "> \n",
    "> + The feature `slope` can be considered to be ordinal.\n",
    "> \n",
    ">   + `'Downsloping'` represents a `downward slope`.\n",
    ">   + `'flat'` represents `no significant slope`.\n",
    ">   + `'upsloping'` represents an `upward slope`.\n",
    ">   + `This ordering` implies a natural progression `from downsloping to flat to upsloping`.\n",
    ">\n",
    "> + The feature `thal` can be considered to be ordinal.\n",
    "> \n",
    ">   + `'normal'` represents `no abnormality`.\n",
    ">   + `'reversible defect'` indicates a `potentially reversible abnormality`.\n",
    ">   + `'Fixed defect'` represents a `permanent abnormality`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;<a id=\"inspect_num_cols\">**6.4 - Inspect Numerical Features**</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id', 'age', 'trestbps', 'chol', 'thalch', 'oldpeak', 'ca', 'num']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a list of numerical columns.\n",
    "num_cols = [col for col in df.columns if df[col].dtype != 'object' and df[col].dtype != 'category']\n",
    "num_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  3.,  2.,  1., nan])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the unique values of each numerical feature one by one.\n",
    "df['ca'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;**Observations - 6.4**\n",
    "> + The feature `ca` considered to be ordinal.\n",
    "> \n",
    ">   + The values `'0'`, `'1'`, `'2'`, and `'3'` can be arranged in a specific order based on the `increasing number of vessels colored`.\n",
    ">   + The ordering from `0 to 3` signifies an `increase in the severity` or extent of the condition being measured."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"data_preprocessing\">7.0 - Data Pre-processing</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;<a id=\"irrelevant_cols\">**7.1 - Drop the Irrelevant Features based on the `Observation - 6.1`**</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the column 'id' from the dataframe.\n",
    "df.drop(['id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;<a id=\"handle_missing\">**7.2 - Handling Missing Data**</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ca          66.4\n",
       "thal        52.8\n",
       "slope       33.6\n",
       "fbs          9.8\n",
       "oldpeak      6.7\n",
       "trestbps     6.4\n",
       "thalch       6.0\n",
       "exang        6.0\n",
       "chol         3.3\n",
       "restecg      0.2\n",
       "dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identify the columns in which the data is missing.\n",
    "round((df.isnull().sum()[df.isnull().sum()>0]/len(df)*100),1).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;<a id=\"handle_num_cols\">**7.2.1 - Impute the `Numerical Features` with low (`less then 10%`) Missing Values by using `IterativeImputer`**</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical features having missing data less than 10% are: ['trestbps', 'chol', 'thalch', 'oldpeak']\n"
     ]
    }
   ],
   "source": [
    "# Finding the numerical columns from the dataframe.\n",
    "num_cols = df.select_dtypes(include=['float', 'int']).columns\n",
    "\n",
    "# Calculating the percentage of missing values in each numerical column\n",
    "missing_percentages = (df[num_cols].isnull().sum() / len(df)) * 100\n",
    "\n",
    "# Select numerical features with missing data less than 10%\n",
    "num_low_missing_cols = missing_percentages[(missing_percentages > 0) & (missing_percentages < 10)].index.tolist()\n",
    "\n",
    "# Print the selected numerical features\n",
    "print(\"Numerical features having missing data less than 10% are:\", num_low_missing_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply IterativeImputer with RandomForestRegressor to impute missing values of low missing numerical columns.\n",
    "iterative_imputer = IterativeImputer(estimator=RandomForestRegressor(random_state=42), random_state=42, add_indicator=True)\n",
    "imputed_values = iterative_imputer.fit_transform(df[num_low_missing_cols])\n",
    "\n",
    "df[num_low_missing_cols] = imputed_values[:, :len(num_low_missing_cols)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ca         66.4\n",
       "thal       52.8\n",
       "slope      33.6\n",
       "fbs         9.8\n",
       "exang       6.0\n",
       "restecg     0.2\n",
       "dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verifying the missing values in low missing numerical columns after imputation.\n",
    "round((df.isnull().sum()[df.isnull().sum()>0]/len(df)*100),1).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ The numerical features had missing values less than 10% have been imputed successfully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;<a id=\"handle_cat_cols\">**7.2.2 - Impute the Categorical Features with low (`less then 10%`) Missing Values by using `Random Values Imputation` technique**</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical features having missing data less than 10% are: ['fbs', 'restecg', 'exang']\n"
     ]
    }
   ],
   "source": [
    "# Finding the categorical columns from the dataframe.\n",
    "cat_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "# Calculating the percentage of missing values in each column.\n",
    "missing_percentages = (df[cat_cols].isnull().sum() / len(df)) * 100\n",
    "\n",
    "# Select categorical features with missing data less than 10%.\n",
    "cat_low_missing_cols = missing_percentages[(missing_percentages > 0) & (missing_percentages < 10)].index.tolist()\n",
    "\n",
    "# Print the selected categorical features.\n",
    "print(\"Categorical features having missing data less than 10% are:\", cat_low_missing_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputing the missing values in categorical columns with low percentage of missing values.\n",
    "for col in cat_low_missing_cols:\n",
    "    df[col][df[col].isnull()] = df[col].dropna().sample(df[col].isnull().sum(), replace=True).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ca       66.4\n",
       "thal     52.8\n",
       "slope    33.6\n",
       "dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verifying the missing values in low missing categorical columns after imputation.\n",
    "round((df.isnull().sum()[df.isnull().sum()>0]/len(df)*100),1).sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ The categorical features had missing values less than 10% have been imputed successfully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;<a id=\"handle_high_missing_cols\">**7.2.3 - Impute the Features with `high percentage` of Missing Values by using `RandomForestClassifier`**</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['slope', 'ca', 'thal']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# identify the features with missing values in more than 10%\n",
    "high_missing_cols = df.isnull().sum()[df.isnull().sum() > 0].index.tolist()\n",
    "high_missing_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_high_missing_data(passed_col):\n",
    "    \n",
    "    df_null = df[df[passed_col].isnull()]\n",
    "    df_not_null = df[df[passed_col].notnull()]\n",
    "\n",
    "    X = df_not_null.drop(passed_col, axis=1)\n",
    "    y = df_not_null[passed_col]\n",
    "    \n",
    "    other_missing_cols = [col for col in high_missing_cols if col != passed_col]\n",
    "    \n",
    "    label_encoder = LabelEncoder()\n",
    "\n",
    "    for col in X.columns:\n",
    "        if X[col].dtype == 'object' or X[col].dtype == 'category':\n",
    "            X[col] = label_encoder.fit_transform(X[col])\n",
    "    \n",
    "    iterative_imputer = IterativeImputer(estimator=RandomForestRegressor(random_state=42), add_indicator=True)\n",
    "\n",
    "    for col in other_missing_cols:\n",
    "        if X[col].isnull().sum() > 0:\n",
    "            col_with_missing_values = X[col].values.reshape(-1, 1)\n",
    "            imputed_values = iterative_imputer.fit_transform(col_with_missing_values)\n",
    "            X[col] = imputed_values[:, 0]\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    rf_classifier = RandomForestClassifier()\n",
    "\n",
    "    rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "    acc_score = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    print(\"The feature '\"+ passed_col+ \"' has been imputed with\", round((acc_score * 100), 2), \"accuracy\\n\")\n",
    "\n",
    "    X = df_null.drop(passed_col, axis=1)\n",
    "\n",
    "    for col in X.columns:\n",
    "        if X[col].dtype == 'object' or X[col].dtype == 'category':\n",
    "            X[col] = label_encoder.fit_transform(X[col])\n",
    "\n",
    "    for col in other_missing_cols:\n",
    "        if X[col].isnull().sum() > 0:\n",
    "            col_with_missing_values = X[col].values.reshape(-1, 1)\n",
    "            imputed_values = iterative_imputer.fit_transform(col_with_missing_values)\n",
    "            X[col] = imputed_values[:, 0]\n",
    "        else:\n",
    "            pass\n",
    "                \n",
    "    if len(df_null) > 0: \n",
    "        df_null[passed_col] = rf_classifier.predict(X)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    df_combined = pd.concat([df_not_null, df_null])\n",
    "    \n",
    "    return df_combined[passed_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "slope    309\n",
       "ca       611\n",
       "thal     486\n",
       "dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()[df.isnull().sum() > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The feature 'slope' has been imputed with 68.29 accuracy\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The feature 'ca' has been imputed with 64.52 accuracy\n",
      "\n",
      "The feature 'thal' has been imputed with 70.11 accuracy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for col in high_missing_cols:\n",
    "    df[col] = impute_high_missing_data(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;<a id=\"observation_improvements\">**Observations and Improvements - 7.2**</a>\n",
    "> + One of my colleague, [Muhammad Bilal Khan](https://www.kaggle.com/devbilalkhan) also worked on the same dataset to impute the missing values, See [Notbook](https://www.kaggle.com/code/devbilalkhan/ml-heart-disease-detection-random-forest/notebook).\n",
    ">\n",
    "> + `'Muhammad Bilal Khan'` used the `SimpleImputer` for the features having `low percentages (less than 10%) of missing data`, with `mean` strategy for `numerical features` and `most_frequent` strategy for the `categorical features`.\n",
    "> \n",
    "> \n",
    "> + I used the `Multivariate Iterative Imputer` for the features having `low percentages (less than 10%) of missing data` with the parameter `add_indicator=True` to avoid the noisiness of the missing data in other features.\n",
    "> \n",
    "> \n",
    "> + I selected the `Non-Null Random Values` of the `categorical features` to impute the missing data within the `corresponding categorical features`, bucause the selection of the 'non-null random values' depends on the `probability of the occurance` of the categories.\n",
    ">   + Although this strategy is not very perfect, but it's better way to `avoid the skewness or biasness` created by `most_frequent` values.\n",
    "> \n",
    "> \n",
    "> + `'Muhammad Bilal Khan'` used the `RandomForestClassifier` for the features having `high percentages of missing data`, but he dropped the other remaining features with high percentages of missing data while imputing first and second featues.\n",
    ">   + Although, this strategy increase accuracy of the imputation of the high percentage of missing data, but the drwaback of this strategy that we are totally `ignoring some featrues` from the dataset while imputing one of them.\n",
    ">   + Ignoring the feature can negatively effect the `accuracy` of the final prediction of the `target feature (num).\n",
    "> \n",
    "> \n",
    "> + To overcome the immediate-above stated issue, instead of dropping the remaing features having high percentages of missing data, I followed the following strategy:\n",
    ">   + I temporarily imputed the featues having high missing data by using the `multivariate iterative imputer`, just before applying the `RandomForestClassifier` while imputing one of them and also updated the imputed feature into the original dataframe.\n",
    "> \n",
    ">   + Although, by following this technique, the accuracy of the imputation of the features having high missing data is `little bit less` than the accuracy acheived `Muhammad Bilal Khan` in his [`notebook`](https://www.kaggle.com/code/devbilalkhan/ml-heart-disease-detection-random-forest/notebook), but this imputation technique considering all the features and it may `positively effect the final prediction` of the `target feature (num)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;**Further Possible Improvements - 7.2**\n",
    "> + We can crate small functions separatly for `temporary imputation` and the `features encoding`, while imputing the missing values in the features having high percentages of missing data.\n",
    "> \n",
    "> + In this way, we can `improve the code reuseability` and `reduce the time-complexity` of the programe."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
